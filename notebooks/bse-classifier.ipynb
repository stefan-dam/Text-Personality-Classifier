{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f96097b8",
      "metadata": {
        "id": "f96097b8"
      },
      "source": [
        "# Big-5 Personality Classifier Training\n",
        "\n",
        "This notebook trains a dual-embedding (RoBERTa + Sentence-BERT) model to classify personality traits from text.\n",
        "\n",
        "**Dataset:** essays-big5 from Hugging Face\n",
        "\n",
        "**Target:** Binary classification for each Big-5 trait (0=low, 1=high)\n",
        "- **E**xtraversion\n",
        "- **N**euroticism\n",
        "- **A**greeableness\n",
        "- **C**onscientiousness\n",
        "- **O**penness\n",
        "\n",
        "**Runtime:** Select **GPU** for faster training (Runtime → Change runtime type → T4 GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fafbd5e9",
      "metadata": {
        "id": "fafbd5e9"
      },
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ba9b9db7",
      "metadata": {
        "id": "ba9b9db7"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers sentence-transformers datasets scikit-learn torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d077e25",
      "metadata": {
        "id": "4d077e25"
      },
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "56958759",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56958759",
        "outputId": "6ccf1461-4418-4f29-bfc8-f318e9ad2187"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6ac1132",
      "metadata": {
        "id": "c6ac1132"
      },
      "source": [
        "## 3. Data Processor\n",
        "\n",
        "Loads the essays-big5 dataset and prepares it for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "34f52849",
      "metadata": {
        "id": "34f52849"
      },
      "outputs": [],
      "source": [
        "class DataProcessor:\n",
        "    \"\"\"\n",
        "    Processes the essays-big5 dataset for personality classification.\n",
        "\n",
        "    Binary labels (0=low trait, 1=high trait) for multi-label classification.\n",
        "    Each person can be high (1) or low (0) on each trait independently.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.trait_names = ['Extraversion', 'Neuroticism', 'Agreeableness',\n",
        "                           'Conscientiousness', 'Openness']\n",
        "\n",
        "    def load_and_preprocess(self):\n",
        "        \"\"\"\n",
        "        Load and preprocess the essays-big5 dataset from Hugging Face.\n",
        "\n",
        "        Returns:\n",
        "            X_train, X_test: Essay texts\n",
        "            y_train, y_test: Big-5 personality scores (binary: 0 or 1)\n",
        "                             Shape: (n_samples, 5) for [E, N, A, C, O]\n",
        "        \"\"\"\n",
        "        print(\"Loading dataset from Hugging Face...\")\n",
        "        ds = load_dataset(\"jingjietan/essays-big5\", split=\"train\")\n",
        "        texts = list(ds[\"text\"]) # Convert to list to ensure compatibility with sklearn's train_test_split\n",
        "\n",
        "        # Extract Big-5 binary labels in OCEAN order\n",
        "        # Values are binary: 0 (low on trait) or 1 (high on trait)\n",
        "        big5_scores = np.array([\n",
        "            [float(row[\"E\"]), float(row[\"N\"]), float(row[\"A\"]),\n",
        "             float(row[\"C\"]), float(row[\"O\"])]\n",
        "            for row in ds\n",
        "        ])\n",
        "\n",
        "        print(f\"Dataset size: {len(texts)} essays\")\n",
        "        print(f\"Label distribution (% high on each trait):\")\n",
        "        for i, trait in enumerate(self.trait_names):\n",
        "            pct = np.mean(big5_scores[:, i]) * 100\n",
        "            print(f\"  {trait}: {pct:.1f}%\")\n",
        "\n",
        "        # Split into train/test sets (80/20)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            texts, big5_scores, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9687253",
      "metadata": {
        "id": "e9687253"
      },
      "source": [
        "## 4. Model Architecture\n",
        "\n",
        "Dual-embedding model combining RoBERTa and Sentence-BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1174e6e5",
      "metadata": {
        "id": "1174e6e5"
      },
      "outputs": [],
      "source": [
        "class PersonalityClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Dual-embedding personality classifier for Big-5 trait prediction.\n",
        "\n",
        "    - RoBERTa: Captures token-level context and linguistic patterns\n",
        "    - Sentence-BERT: Captures semantic meaning and overall coherence\n",
        "    - Frozen base models to prevent overfitting\n",
        "    - Output: 5 binary predictions for [E, N, A, C, O]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name='roberta-base'):\n",
        "        super(PersonalityClassifier, self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(model_name)\n",
        "        self.sbert = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "        # Freeze RoBERTa parameters to leverage pre-trained knowledge\n",
        "        for param in self.roberta.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Combined embedding dimension (RoBERTa + sBERT)\n",
        "        roberta_dim = 768\n",
        "        sbert_dim = 768\n",
        "        combined_dim = roberta_dim + sbert_dim\n",
        "\n",
        "        # Classifier layers for Big-5 personality prediction\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(combined_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 5)  # Output: 5 Big-5 traits [E, N, A, C, O]\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, texts):\n",
        "        # Get RoBERTa embeddings using [CLS] token\n",
        "        roberta_outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
        "        roberta_embeddings = roberta_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Get sBERT embeddings\n",
        "        sbert_embeddings = torch.tensor(self.sbert.encode(texts)).to(input_ids.device)\n",
        "\n",
        "        # Concatenate embeddings\n",
        "        combined_embeddings = torch.cat([roberta_embeddings, sbert_embeddings], dim=1)\n",
        "\n",
        "        # Pass through classifier to predict Big-5 traits\n",
        "        big5_predictions = self.classifier(combined_embeddings)\n",
        "\n",
        "        return big5_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac167452",
      "metadata": {
        "id": "ac167452"
      },
      "source": [
        "## 5. Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2d2ff2cc",
      "metadata": {
        "id": "2d2ff2cc"
      },
      "outputs": [],
      "source": [
        "class TextPersonalityDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': torch.tensor(label, dtype=torch.float32)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88f5023a",
      "metadata": {
        "id": "88f5023a"
      },
      "source": [
        "## 6. Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f5bed891",
      "metadata": {
        "id": "f5bed891"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def train_model(model, train_loader, val_loader, device, num_epochs=10):\n",
        "    \"\"\"\n",
        "    Train the personality classifier for binary multi-label prediction.\n",
        "    \"\"\"\n",
        "\n",
        "    # loss for independent binary labels\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=1\n",
        "    )\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 2\n",
        "    wait = 0\n",
        "\n",
        "    trait_names = [\n",
        "        'Extraversion', 'Neuroticism', 'Agreeableness',\n",
        "        'Conscientiousness', 'Openness'\n",
        "    ]\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # -----------------------------\n",
        "        # TRAINING\n",
        "        # -----------------------------\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].float().to(device)   # ensure float\n",
        "            texts = batch['text']\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits = model(input_ids, attention_mask, texts)  # raw logits\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # -----------------------------\n",
        "        # VALIDATION\n",
        "        # -----------------------------\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_logits = []\n",
        "        all_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].float().to(device)\n",
        "                texts = batch['text']\n",
        "\n",
        "                logits = model(input_ids, attention_mask, texts)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                all_logits.append(logits.cpu().numpy())\n",
        "                all_targets.append(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        # -----------------------------\n",
        "        # METRICS\n",
        "        # -----------------------------\n",
        "        logits = np.concatenate(all_logits, axis=0)\n",
        "        targets = np.concatenate(all_targets, axis=0)\n",
        "\n",
        "        # Convert logits → probabilities\n",
        "        probs = 1 / (1 + np.exp(-logits))\n",
        "        preds = (probs > 0.5).astype(int)\n",
        "\n",
        "        # Binary accuracy\n",
        "        binary_accuracy = np.mean(preds == targets) * 100\n",
        "\n",
        "        # Per-trait accuracy\n",
        "        per_trait_accuracy = np.mean(preds == targets, axis=0) * 100\n",
        "\n",
        "        # F1 score (better than accuracy for imbalanced traits)\n",
        "        f1 = f1_score(targets, preds, average='micro')\n",
        "\n",
        "        # -----------------------------\n",
        "        # PRINT METRICS\n",
        "        # -----------------------------\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"  Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "        print(f\"  Binary Accuracy: {binary_accuracy:.2f}% | F1: {f1:.4f}\")\n",
        "        print(\"  Per-trait Accuracy: \", end=\"\")\n",
        "        for i, (name, acc) in enumerate(zip(trait_names, per_trait_accuracy)):\n",
        "            print(f\"{name[:3]}={acc:.1f}%\", end=\" \" if i < 4 else \"\\n\")\n",
        "\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        # -----------------------------\n",
        "        # CHECKPOINTS\n",
        "        # -----------------------------\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "            print(f\"  ✓ Saved New Best Model (val_loss={best_val_loss:.4f})\")\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(f\"\\nEarly stopping at epoch {epoch+1} — no improvement.\")\n",
        "                break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2728ce1",
      "metadata": {
        "id": "d2728ce1"
      },
      "source": [
        "## 7. Main Training Pipeline\n",
        "\n",
        "**Note:** This will take approximately:\n",
        "- **With GPU (T4)**: ~15-20 minutes\n",
        "- **Without GPU (CPU)**: ~2-3 hours\n",
        "\n",
        "Make sure to enable GPU runtime for faster training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fb2568a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb2568a0",
        "outputId": "b74e0836-8fa3-4792-dac7-827d3b324dc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Big-5 Personality Binary Classifier Training\n",
            "============================================================\n",
            "\n",
            "[1/5] Loading and preprocessing data...\n",
            "Loading dataset from Hugging Face...\n",
            "Dataset size: 1578 essays\n",
            "Label distribution (% high on each trait):\n",
            "  Extraversion: 51.8%\n",
            "  Neuroticism: 49.9%\n",
            "  Agreeableness: 53.0%\n",
            "  Conscientiousness: 50.8%\n",
            "  Openness: 51.5%\n",
            "  Dataset: 1262 training samples, 316 test samples\n",
            "  Target: Binary Big-5 traits (0=low, 1=high) [E, N, A, C, O]\n",
            "  Task: Multi-label binary classification\n",
            "\n",
            "[2/5] Initializing model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Device: cuda\n",
            "  Trainable parameters: 110,406,021 / 235,051,653 (47.0%)\n",
            "\n",
            "[3/5] Creating data loaders...\n",
            "  Batch size: 16\n",
            "  Training batches: 40\n",
            "  Validation batches: 10\n",
            "\n",
            "[4/5] Training model...\n",
            "------------------------------------------------------------\n",
            "\n",
            "Epoch 1/10\n",
            "  Train Loss: 0.6935 | Val Loss: 0.6920\n",
            "  Binary Accuracy: 52.28% | F1: 0.6423\n",
            "  Per-trait Accuracy: Ext=57.0% Neu=47.2% Agr=56.3% Con=53.8% Ope=47.2%\n",
            "  ✓ Saved New Best Model (val_loss=0.6920)\n",
            "\n",
            "Epoch 2/10\n",
            "  Train Loss: 0.6929 | Val Loss: 0.6916\n",
            "  Binary Accuracy: 54.81% | F1: 0.6301\n",
            "  Per-trait Accuracy: Ext=57.0% Neu=57.0% Agr=56.3% Con=52.5% Ope=51.3%\n",
            "  ✓ Saved New Best Model (val_loss=0.6916)\n",
            "\n",
            "Epoch 3/10\n",
            "  Train Loss: 0.6926 | Val Loss: 0.6914\n",
            "  Binary Accuracy: 55.82% | F1: 0.6667\n",
            "  Per-trait Accuracy: Ext=57.0% Neu=57.0% Agr=56.3% Con=54.4% Ope=54.4%\n",
            "  ✓ Saved New Best Model (val_loss=0.6914)\n",
            "\n",
            "Epoch 4/10\n",
            "  Train Loss: 0.6921 | Val Loss: 0.6912\n",
            "  Binary Accuracy: 54.62% | F1: 0.6691\n",
            "  Per-trait Accuracy: Ext=57.0% Neu=47.2% Agr=56.3% Con=55.1% Ope=57.6%\n",
            "  ✓ Saved New Best Model (val_loss=0.6912)\n",
            "\n",
            "Epoch 5/10\n",
            "  Train Loss: 0.6922 | Val Loss: 0.6913\n",
            "  Binary Accuracy: 53.23% | F1: 0.6532\n",
            "  Per-trait Accuracy: Ext=57.6% Neu=47.2% Agr=56.3% Con=49.4% Ope=55.7%\n",
            "\n",
            "Epoch 6/10\n",
            "  Train Loss: 0.6916 | Val Loss: 0.6909\n",
            "  Binary Accuracy: 55.38% | F1: 0.6741\n",
            "  Per-trait Accuracy: Ext=61.1% Neu=47.2% Agr=56.3% Con=54.7% Ope=57.6%\n",
            "  ✓ Saved New Best Model (val_loss=0.6909)\n",
            "\n",
            "Epoch 7/10\n",
            "  Train Loss: 0.6914 | Val Loss: 0.6904\n",
            "  Binary Accuracy: 54.94% | F1: 0.6567\n",
            "  Per-trait Accuracy: Ext=58.9% Neu=50.0% Agr=56.3% Con=53.8% Ope=55.7%\n",
            "  ✓ Saved New Best Model (val_loss=0.6904)\n",
            "\n",
            "Epoch 8/10\n",
            "  Train Loss: 0.6907 | Val Loss: 0.6902\n",
            "  Binary Accuracy: 55.06% | F1: 0.6468\n",
            "  Per-trait Accuracy: Ext=57.3% Neu=52.5% Agr=56.3% Con=52.2% Ope=57.0%\n",
            "  ✓ Saved New Best Model (val_loss=0.6902)\n",
            "\n",
            "Epoch 9/10\n",
            "  Train Loss: 0.6901 | Val Loss: 0.6894\n",
            "  Binary Accuracy: 55.38% | F1: 0.6609\n",
            "  Per-trait Accuracy: Ext=58.9% Neu=50.9% Agr=56.3% Con=54.4% Ope=56.3%\n",
            "  ✓ Saved New Best Model (val_loss=0.6894)\n",
            "\n",
            "Epoch 10/10\n",
            "  Train Loss: 0.6892 | Val Loss: 0.6887\n",
            "  Binary Accuracy: 55.70% | F1: 0.6418\n",
            "  Per-trait Accuracy: Ext=58.2% Neu=55.7% Agr=56.3% Con=53.8% Ope=54.4%\n",
            "  ✓ Saved New Best Model (val_loss=0.6887)\n",
            "\n",
            "[5/5] Training complete!\n",
            "  Best model saved to: best_model.pt\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    print(\"=\"*60)\n",
        "    print(\"Big-5 Personality Binary Classifier Training\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Initialize data processor\n",
        "    print(\"\\n[1/5] Loading and preprocessing data...\")\n",
        "    data_processor = DataProcessor()\n",
        "    X_train, X_test, y_train, y_test = data_processor.load_and_preprocess()\n",
        "    print(f\"  Dataset: {len(X_train)} training samples, {len(X_test)} test samples\")\n",
        "    print(f\"  Target: Binary Big-5 traits (0=low, 1=high) [E, N, A, C, O]\")\n",
        "    print(f\"  Task: Multi-label binary classification\")\n",
        "\n",
        "    # Initialize tokenizer and model\n",
        "    print(\"\\n[2/5] Initializing model...\")\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "    model = PersonalityClassifier()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Count trainable parameters\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"  Device: {device}\")\n",
        "    print(f\"  Trainable parameters: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.1f}%)\")\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    print(\"\\n[3/5] Creating data loaders...\")\n",
        "    train_dataset = TextPersonalityDataset(X_train, y_train, tokenizer)\n",
        "    test_dataset = TextPersonalityDataset(X_test, y_test, tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "    print(f\"  Batch size: 16\")\n",
        "    print(f\"  Training batches: {len(train_loader)}\")\n",
        "    print(f\"  Validation batches: {len(test_loader)}\")\n",
        "\n",
        "    # Train the model\n",
        "    print(\"\\n[4/5] Training model...\")\n",
        "    print(\"-\"*60)\n",
        "    train_model(model, train_loader, test_loader, device)\n",
        "\n",
        "    print(\"\\n[5/5] Training complete!\")\n",
        "    print(\"  Best model saved to: best_model.pt\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# Run the training\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45341f89",
      "metadata": {
        "id": "45341f89"
      },
      "source": [
        "## 8. Download Trained Model\n",
        "\n",
        "Download the trained model to your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "23f1b9f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "23f1b9f8",
        "outputId": "4f50d0b9-6db2-4bf5-91e8-89affe04df3c"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_dfbebe90-ff43-4c2c-92ce-d7e307a1b618\", \"best_model.pt\", 940377765)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the best model\n",
        "files.download('best_model.pt')\n",
        "\n",
        "# Optionally download epoch checkpoints\n",
        "# files.download('weights_epoch_3.pt')\n",
        "# files.download('weights_epoch_4.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cb7bfae",
      "metadata": {
        "id": "8cb7bfae"
      },
      "source": [
        "## 9. Test the Model (Validating)\n",
        "\n",
        "Make predictions on custom text samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a3f7b92",
      "metadata": {
        "id": "2a3f7b92"
      },
      "outputs": [],
      "source": [
        "def predict_personality(text, model, tokenizer, device):\n",
        "    \"\"\"\n",
        "    Predict personality traits from text using the trained model.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        max_length=128,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids, attention_mask, [text])\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
        "        preds = (probs > 0.5).astype(int)\n",
        "\n",
        "    trait_names = ['Extraversion', 'Neuroticism', 'Agreeableness', 'Conscientiousness', 'Openness']\n",
        "\n",
        "    print(\"\\nPredicted Personality Profile:\")\n",
        "    print(\"-\" * 50)\n",
        "    for trait, pred, score in zip(trait_names, preds, probs):\n",
        "        level = \"HIGH\" if pred == 1 else \"LOW\"\n",
        "        print(f\"{trait:18} : {level:4} (prob={score:.3f})\")\n",
        "\n",
        "# 1. Load tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# 2. Load trained model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = PersonalityClassifier()\n",
        "model.load_state_dict(torch.load(\"best_model.pt\", map_location=device))\n",
        "model.to(device)\n",
        "\n",
        "# 3. Predict\n",
        "predict_personality(\n",
        "    text=\"I am happy and excited to go to the event\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
