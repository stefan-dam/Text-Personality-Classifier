{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f96097b8",
   "metadata": {},
   "source": [
    "# Big-5 Personality Classifier Training\n",
    "\n",
    "This notebook trains a dual-embedding (RoBERTa + Sentence-BERT) model to classify personality traits from text.\n",
    "\n",
    "**Dataset:** essays-big5 from Hugging Face\n",
    "\n",
    "**Target:** Binary classification for each Big-5 trait (0=low, 1=high)\n",
    "- **E**xtraversion\n",
    "- **N**euroticism\n",
    "- **A**greeableness\n",
    "- **C**onscientiousness\n",
    "- **O**penness\n",
    "\n",
    "**Runtime:** Select **GPU** for faster training (Runtime → Change runtime type → T4 GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafbd5e9",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9b9db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers sentence-transformers datasets scikit-learn torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d077e25",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56958759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac1132",
   "metadata": {},
   "source": [
    "## 3. Data Processor\n",
    "\n",
    "Loads the essays-big5 dataset and prepares it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f52849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    Processes the essays-big5 dataset for personality classification.\n",
    "    \n",
    "    Binary labels (0=low trait, 1=high trait) for multi-label classification.\n",
    "    Each person can be high (1) or low (0) on each trait independently.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.trait_names = ['Extraversion', 'Neuroticism', 'Agreeableness', \n",
    "                           'Conscientiousness', 'Openness']\n",
    "\n",
    "    def load_and_preprocess(self):\n",
    "        \"\"\"\n",
    "        Load and preprocess the essays-big5 dataset from Hugging Face.\n",
    "        \n",
    "        Returns:\n",
    "            X_train, X_test: Essay texts\n",
    "            y_train, y_test: Big-5 personality scores (binary: 0 or 1)\n",
    "                             Shape: (n_samples, 5) for [E, N, A, C, O]\n",
    "        \"\"\"\n",
    "        print(\"Loading dataset from Hugging Face...\")\n",
    "        ds = load_dataset(\"jingjietan/essays-big5\", split=\"train\")\n",
    "        texts = ds[\"text\"]\n",
    "        \n",
    "        # Extract Big-5 binary labels in OCEAN order\n",
    "        # Values are binary: 0 (low on trait) or 1 (high on trait)\n",
    "        big5_scores = np.array([\n",
    "            [float(row[\"E\"]), float(row[\"N\"]), float(row[\"A\"]), \n",
    "             float(row[\"C\"]), float(row[\"O\"])]\n",
    "            for row in ds\n",
    "        ])\n",
    "        \n",
    "        print(f\"Dataset size: {len(texts)} essays\")\n",
    "        print(f\"Label distribution (% high on each trait):\")\n",
    "        for i, trait in enumerate(self.trait_names):\n",
    "            pct = np.mean(big5_scores[:, i]) * 100\n",
    "            print(f\"  {trait}: {pct:.1f}%\")\n",
    "        \n",
    "        # Split into train/test sets (80/20)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            texts, big5_scores, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9687253",
   "metadata": {},
   "source": [
    "## 4. Model Architecture\n",
    "\n",
    "Dual-embedding model combining RoBERTa and Sentence-BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1174e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonalityClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Dual-embedding personality classifier for Big-5 trait prediction.\n",
    "    \n",
    "    - RoBERTa: Captures token-level context and linguistic patterns\n",
    "    - Sentence-BERT: Captures semantic meaning and overall coherence\n",
    "    - Frozen base models to prevent overfitting\n",
    "    - Output: 5 binary predictions for [E, N, A, C, O]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='roberta-base'):\n",
    "        super(PersonalityClassifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(model_name)\n",
    "        self.sbert = SentenceTransformer('all-mpnet-base-v2')\n",
    "        \n",
    "        # Freeze RoBERTa parameters to leverage pre-trained knowledge\n",
    "        for param in self.roberta.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Combined embedding dimension (RoBERTa + sBERT)\n",
    "        roberta_dim = 768\n",
    "        sbert_dim = 768\n",
    "        combined_dim = roberta_dim + sbert_dim\n",
    "        \n",
    "        # Classifier layers for Big-5 personality prediction\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 5)  # Output: 5 Big-5 traits [E, N, A, C, O]\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, texts):\n",
    "        # Get RoBERTa embeddings using [CLS] token\n",
    "        roberta_outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        roberta_embeddings = roberta_outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Get sBERT embeddings\n",
    "        sbert_embeddings = torch.tensor(self.sbert.encode(texts)).to(input_ids.device)\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        combined_embeddings = torch.cat([roberta_embeddings, sbert_embeddings], dim=1)\n",
    "        \n",
    "        # Pass through classifier to predict Big-5 traits\n",
    "        big5_predictions = self.classifier(combined_embeddings)\n",
    "        \n",
    "        return big5_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac167452",
   "metadata": {},
   "source": [
    "## 5. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ff2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPersonalityDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float32)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f5023a",
   "metadata": {},
   "source": [
    "## 6. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bed891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Train the personality classifier with comprehensive metrics.\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.05)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience = 2\n",
    "    wait = 0\n",
    "    trait_names = ['Extraversion', 'Neuroticism', 'Agreeableness', 'Conscientiousness', 'Openness']\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            texts = batch['text']\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask, texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                texts = batch['text']\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask, texts)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                val_preds.append(outputs.cpu().numpy())\n",
    "                val_targets.append(labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Calculate detailed metrics\n",
    "        val_preds = np.concatenate(val_preds, axis=0)\n",
    "        val_targets = np.concatenate(val_targets, axis=0)\n",
    "        \n",
    "        # Overall MAE (Mean Absolute Error)\n",
    "        overall_mae = np.mean(np.abs(val_preds - val_targets))\n",
    "        \n",
    "        # Binary classification accuracy (threshold at 0.5)\n",
    "        binary_accuracy = np.mean((val_preds > 0.5).astype(int) == val_targets.astype(int)) * 100\n",
    "        \n",
    "        # Per-trait binary accuracy\n",
    "        per_trait_accuracy = np.mean((val_preds > 0.5).astype(int) == val_targets.astype(int), axis=0) * 100\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Val MAE: {overall_mae:.4f} | Binary Accuracy: {binary_accuracy:.2f}%\")\n",
    "        print(f\"  Per-trait Accuracy: \", end=\"\")\n",
    "        for i, (name, acc) in enumerate(zip(trait_names, per_trait_accuracy)):\n",
    "            print(f\"{name[:3]}={acc:.1f}%\", end=\" \" if i < 4 else \"\\n\")\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Save checkpoints at specific epochs for analysis\n",
    "        if epoch+1 in [3, 4]:\n",
    "            torch.save(model.state_dict(), f'weights_epoch_{epoch+1}.pt')\n",
    "            print(f\"  Saved checkpoint: weights_epoch_{epoch+1}.pt\")\n",
    "        \n",
    "        # Early stopping with best model saving\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(f\"  ✓ New best model saved (val_loss: {best_val_loss:.4f})\")\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1} (no improvement for {patience} epochs)\")\n",
    "                break\n",
    "        \n",
    "        print()  # Blank line for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2728ce1",
   "metadata": {},
   "source": [
    "## 7. Main Training Pipeline\n",
    "\n",
    "**Note:** This will take approximately:\n",
    "- **With GPU (T4)**: ~15-20 minutes\n",
    "- **Without GPU (CPU)**: ~2-3 hours\n",
    "\n",
    "Make sure to enable GPU runtime for faster training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2568a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(\"Big-5 Personality Binary Classifier Training\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize data processor\n",
    "    print(\"\\n[1/5] Loading and preprocessing data...\")\n",
    "    data_processor = DataProcessor()\n",
    "    X_train, X_test, y_train, y_test = data_processor.load_and_preprocess()\n",
    "    print(f\"  Dataset: {len(X_train)} training samples, {len(X_test)} test samples\")\n",
    "    print(f\"  Target: Binary Big-5 traits (0=low, 1=high) [E, N, A, C, O]\")\n",
    "    print(f\"  Task: Multi-label binary classification\")\n",
    "    \n",
    "    # Initialize tokenizer and model\n",
    "    print(\"\\n[2/5] Initializing model...\")\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    model = PersonalityClassifier()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"  Device: {device}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.1f}%)\")\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    print(\"\\n[3/5] Creating data loaders...\")\n",
    "    train_dataset = TextPersonalityDataset(X_train, y_train, tokenizer)\n",
    "    test_dataset = TextPersonalityDataset(X_test, y_test, tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "    print(f\"  Batch size: 16\")\n",
    "    print(f\"  Training batches: {len(train_loader)}\")\n",
    "    print(f\"  Validation batches: {len(test_loader)}\")\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"\\n[4/5] Training model...\")\n",
    "    print(\"-\"*60)\n",
    "    train_model(model, train_loader, test_loader, device)\n",
    "    \n",
    "    print(\"\\n[5/5] Training complete!\")\n",
    "    print(\"  Best model saved to: best_model.pt\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Run the training\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45341f89",
   "metadata": {},
   "source": [
    "## 8. Download Trained Model (Optional)\n",
    "\n",
    "Download the trained model to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f1b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Download the best model\n",
    "files.download('best_model.pt')\n",
    "\n",
    "# Optionally download epoch checkpoints\n",
    "# files.download('weights_epoch_3.pt')\n",
    "# files.download('weights_epoch_4.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb7bfae",
   "metadata": {},
   "source": [
    "## 9. Test the Model (Optional)\n",
    "\n",
    "Make predictions on custom text samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3f7b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_personality(text, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Predict personality traits from text.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask, [text])\n",
    "        predictions = (outputs.cpu().numpy() > 0.5).astype(int)[0]\n",
    "        scores = outputs.cpu().numpy()[0]\n",
    "    \n",
    "    # Display results\n",
    "    trait_names = ['Extraversion', 'Neuroticism', 'Agreeableness', 'Conscientiousness', 'Openness']\n",
    "    print(\"\\nPredicted Personality Profile:\")\n",
    "    print(\"-\" * 50)\n",
    "    for trait, pred, score in zip(trait_names, predictions, scores):\n",
    "        level = \"HIGH\" if pred == 1 else \"LOW\"\n",
    "        print(f\"{trait:18} : {level:4} (score: {score:.3f})\")\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"\"\"I absolutely love meeting new people and going to parties. \n",
    "I'm always the center of attention and I thrive in social situations. \n",
    "I get energized by being around others and hate being alone.\"\"\"\n",
    "\n",
    "predict_personality(sample_text, model, tokenizer, device)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
